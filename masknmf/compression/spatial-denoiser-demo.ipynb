{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68305a7a-60c6-4d07-a624-2957bf0a8c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No windowing system present. Using surfaceless platform\n",
      "No config found!\n",
      "No config found!\n",
      "Max vertex attribute stride unknown. Assuming it is 2048\n",
      "Max vertex attribute stride unknown. Assuming it is 2048\n",
      "Max vertex attribute stride unknown. Assuming it is 2048\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f051044353b43fe8020353bb2c96796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x01,\\x00\\x00\\x007\\x08\\x06\\x00\\x00\\x00\\xb6\\x1bw\\x99\\x‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Available devices:</b><table><tr><th>Valid</th><th>Device</th><th>Type</th><th>Backend</th><th>Driver</th></tr><tr title=\"This adapter cannot be used with fastplotlib\"><td>‚ùå (default) </td><td>NVIDIA A100-PCIE-40GB/PCIe/SSE2</td><td>Unknown</td><td>OpenGL</td><td>3.3.0 NVIDIA 565.57.01</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Max vertex attribute stride unknown. Assuming it is 2048\n",
      "Max vertex attribute stride unknown. Assuming it is 2048\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from omegaconf import OmegaConf\n",
    "import masknmf\n",
    "from typing import *\n",
    "from spatial_denoiser import train_spatial_denoiser\n",
    "from spatial_denoiser import create_pmd_denoiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efcd9bbe-6c82-4493-83d1-531d9128c594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded data with shape (T,H,W): (11700, 500, 620)\n",
      "[25-12-11 05:35:26]: Starting compression\n",
      "[25-12-11 05:35:26]: sampled from the following regions: [0]\n",
      "[25-12-11 05:35:26]: We are initializing on a total of 11700 frames\n",
      "[25-12-11 05:35:35]: Loading data to estimate complete spatial basis\n",
      "[25-12-11 05:35:35]: skipping the pruning step for frame cutoff\n",
      "[25-12-11 05:35:35]: Finding spatiotemporal roughness thresholds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:01<00:00, 234.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25-12-11 05:35:36]: Running Blockwise Decompositions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25-12-11 05:36:25]: Constructed U matrix. Rank of U is 2563\n",
      "[25-12-11 05:36:25]: PMD Objected constructed\n",
      "PMD rank: 2563\n",
      "Spatial components shape: torch.Size([500, 620, 2563])\n",
      "Number of components: 2563\n"
     ]
    }
   ],
   "source": [
    "class MotionBinDataset:\n",
    "    \"\"\"Load a memmapped suite2p data.bin together with metadata (.npy/.zip).\"\"\"\n",
    "    def __init__(self, data_path: str, metadata_path: str, dtype=np.int16):\n",
    "        self.bin_path = Path(data_path)\n",
    "        self.ops_path = Path(metadata_path)\n",
    "        self._dtype = dtype\n",
    "        self._shape = self._compute_shape()\n",
    "        self.data = np.memmap(self.bin_path, mode='r', dtype=self._dtype, shape=self._shape)\n",
    "\n",
    "    def _compute_shape(self) -> Tuple[int, int, int]:\n",
    "        _, ext = os.path.splitext(self.ops_path)\n",
    "        if ext == \".zip\":\n",
    "            ops = np.load(self.ops_path, allow_pickle=True)['ops'].item()\n",
    "        elif ext == \".npy\":\n",
    "            ops = np.load(self.ops_path, allow_pickle=True).item()\n",
    "        else:\n",
    "            raise ValueError(\"Metadata file must be .zip or .npy\")\n",
    "        return int(ops['nframes']), int(ops['Ly']), int(ops['Lx'])\n",
    "\n",
    "    @property\n",
    "    def shape(self) -> Tuple[int, int, int]:\n",
    "        return self._shape\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.data[item].copy()\n",
    "\n",
    "config = {\n",
    "    'bin_file_path': '/burg-archive/home/lm3879/plane4/data.bin',\n",
    "    'ops_file_path': '/burg-archive/home/lm3879/plane4/ops.npy',\n",
    "    'out_path': '/burg-archive/home/lm3879/masknmf-toolbox/ibl_denoised_output/pmd_spatial_results.npz',\n",
    "    'block_size_dim1': 32,\n",
    "    'block_size_dim2': 32,\n",
    "    'max_components': 20,\n",
    "    'max_consecutive_failures': 1,\n",
    "    'spatial_avg_factor': 1,\n",
    "    'temporal_avg_factor': 1,\n",
    "    'device': 'cuda',\n",
    "    'frame_batch_size': 1024,\n",
    "    'denoiser_max_epochs': 5,\n",
    "    'denoiser_batch_size': 32,\n",
    "    'denoiser_lr': 1e-4,\n",
    "    'patch_h': 40,\n",
    "    'patch_w': 40,\n",
    "}\n",
    "\n",
    "cfg = OmegaConf.create(config)\n",
    "device = cfg.device if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "my_data = MotionBinDataset(cfg.bin_file_path, cfg.ops_file_path)\n",
    "print(f\"Loaded data with shape (T,H,W): {my_data.shape}\")\n",
    "binary_mask = np.zeros((my_data.shape[1], my_data.shape[2]), dtype=np.float32)\n",
    "binary_mask[3:-3, 3:-3] = 1.0\n",
    "\n",
    "pmd_no_denoise = masknmf.compression.pmd_decomposition(\n",
    "    my_data,\n",
    "    [cfg.block_size_dim1, cfg.block_size_dim2],\n",
    "    my_data.shape[0],\n",
    "    max_components=cfg.max_components,\n",
    "    max_consecutive_failures=cfg.max_consecutive_failures,\n",
    "    temporal_avg_factor=cfg.temporal_avg_factor,\n",
    "    spatial_avg_factor=cfg.spatial_avg_factor,\n",
    "    device=cfg.device,\n",
    "    temporal_denoiser=None,\n",
    "    frame_batch_size=cfg.frame_batch_size,\n",
    "    pixel_weighting=binary_mask\n",
    ")\n",
    "print(f\"PMD rank: {pmd_no_denoise.pmd_rank}\")\n",
    "u_dense = pmd_no_denoise.u.to_dense()\n",
    "H, W = my_data.shape[1], my_data.shape[2]\n",
    "u_reshaped = u_dense.reshape(H, W, -1)\n",
    "print(f\"Spatial components shape: {u_reshaped.shape}\")\n",
    "print(f\"Number of components: {u_reshaped.shape[2]}\")\n",
    "spatial_components = u_reshaped.permute(2, 0, 1)  # (rank, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52485a9d-6dda-4579-8f3f-868d082dc711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting Spatial Denoiser Training\n",
      "============================================================\n",
      "Device: cuda\n",
      "GPU: NVIDIA A100-PCIE-40GB\n",
      "Total memory: 39.50 GB\n",
      "\n",
      "============================================================\n",
      "Extracting valid patches from spatial components...\n",
      "============================================================\n",
      "  Processed 500/2563 components...\n",
      "  Processed 1000/2563 components...\n",
      "  Processed 1500/2563 components...\n",
      "  Processed 2000/2563 components...\n",
      "  Processed 2500/2563 components...\n",
      "\n",
      "Extracted 2563 valid patches from 2563 components\n",
      "============================================================\n",
      "\n",
      "\n",
      "üìä Patch Statistics:\n",
      "  Number of patches: 2563\n",
      "  Size range: 34 - 36 pixels\n",
      "  Average size: 1293 pixels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "/burg-archive/home/lm3879/miniconda3/envs/py311v2/lib/python3.11/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name            | Type           | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | spatial_network | SpatialNetwork | 509 K  | train\n",
      "-----------------------------------------------------------\n",
      "509 K     Trainable params\n",
      "0         Non-trainable params\n",
      "509 K     Total params\n",
      "2.037     Total estimated model params size (MB)\n",
      "73        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created: 2563 patches\n",
      "Memory footprint: 0.01 GB\n",
      "Using 2563 random patches for training\n",
      "DataLoader ready with 81 batches\n",
      "\n",
      "Starting training...\n",
      "Effective batch size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/burg-archive/home/lm3879/miniconda3/envs/py311v2/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751fc81cec95422facc93e7cb8fd983a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n",
      "Peak GPU memory: 0.48 GB\n",
      "Model saved to: denoiser_output/spatial_denoiser_state_dict.pth\n",
      "\n",
      "Blindspot leakage test: 0.00e+00\n",
      "‚úì Blindspot property verified\n",
      "\n",
      "‚úì PMD spatial denoiser created\n",
      "  Noise variance quantile: 0.7\n",
      "  Padding: 12\n",
      "  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "spatial_model, _ = train_spatial_denoiser(\n",
    "    spatial_components,\n",
    "    config={'max_epochs': 5},\n",
    "    output_dir='./denoiser_output'\n",
    ")\n",
    "\n",
    "spatial_denoiser = create_pmd_denoiser(\n",
    "    trained_model=spatial_model,\n",
    "    noise_variance_quantile=0.7,\n",
    "    padding=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9b258bb-45c5-44f0-b43f-c8fe68e7e1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/burg-archive/home/lm3879/miniconda3/envs/py311v2/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:751: Checkpoint directory /burg-archive/home/lm3879/masknmf-toolbox/masknmf/compression/lightning_logs/version_5252547/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\n",
      "  | Name             | Type            | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | temporal_network | TemporalNetwork | 278 K  | train\n",
      "-------------------------------------------------------------\n",
      "170 K     Trainable params\n",
      "107 K     Non-trainable params\n",
      "278 K     Total params\n",
      "1.114     Total estimated model params size (MB)\n",
      "73        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff0543b16044c89b292f4415a3af027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "v = pmd_no_denoise.v.cpu()\n",
    "temporal_model, _ = masknmf.compression.denoising.train_total_variance_denoiser(v,\n",
    "                                                                        max_epochs=5,\n",
    "                                                                        batch_size=128,\n",
    "                                                                        learning_rate=1e-4)\n",
    "\n",
    "\n",
    "temporal_denoiser = masknmf.compression.PMDTemporalDenoiser(temporal_model, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7d6151b-b9ca-42b3-81c6-70ca7ec8c8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25-12-11 05:37:39]: Starting compression\n",
      "[25-12-11 05:37:39]: sampled from the following regions: [0]\n",
      "[25-12-11 05:37:39]: We are initializing on a total of 11700 frames\n",
      "[25-12-11 05:37:45]: Loading data to estimate complete spatial basis\n",
      "[25-12-11 05:37:45]: skipping the pruning step for frame cutoff\n",
      "[25-12-11 05:37:45]: Finding spatiotemporal roughness thresholds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:05<00:00, 45.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25-12-11 05:37:51]: Running Blockwise Decompositions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25-12-11 05:41:00]: Constructed U matrix. Rank of U is 3783\n",
      "[25-12-11 05:41:00]: PMD Objected constructed\n",
      "\n",
      "============================================================\n",
      "PMD Results:\n",
      "  With spatial & temporal denoiser: rank = 3783\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "pmd_with_denoise_S_T = masknmf.compression.pmd_decomposition(\n",
    "    my_data,\n",
    "    [cfg.block_size_dim1, cfg.block_size_dim2],\n",
    "    my_data.shape[0],\n",
    "    max_components=cfg.max_components,\n",
    "    max_consecutive_failures=cfg.max_consecutive_failures,\n",
    "    temporal_avg_factor=cfg.temporal_avg_factor,\n",
    "    spatial_avg_factor=cfg.spatial_avg_factor,\n",
    "    device=cfg.device,\n",
    "    spatial_denoiser=spatial_denoiser,\n",
    "    temporal_denoiser=temporal_denoiser,\n",
    "    frame_batch_size=cfg.frame_batch_size,\n",
    "    pixel_weighting=binary_mask\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PMD Results:\")\n",
    "print(f\"  With spatial & temporal denoiser: rank = {pmd_with_denoise_S_T.pmd_rank}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36047045-d76f-4ddc-bb66-b535376f80ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_arr_Spatial_Temporal = masknmf.compression.PMDResidualArray(my_data, pmd_with_denoise_S_T)\n",
    "import fastplotlib as fpl\n",
    "iw = fpl.ImageWidget(\n",
    "    data=[my_data, pmd_with_denoise_S_T, resid_arr_Spatial_Temporal], \n",
    "    names=['motion corrected', 'spatial and temporal denoised', 'residual']\n",
    ")\n",
    "iw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d05550-53a9-47ea-b6a2-8b9bec5d9e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_dense_before = pmd_no_denoise.u.to_dense().cpu()     # (H*W, R)\n",
    "u_dense_after  = pmd_with_denoise_S_T.u.to_dense().cpu() # (H*W, R)\n",
    "H, W = my_data.shape[1], my_data.shape[2]\n",
    "u_before = u_dense_before.reshape(H, W, -1)    # (H, W, R)\n",
    "u_after  = u_dense_after.reshape(H, W, -1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import label\n",
    "\n",
    "# --------------------- Adjustable Parameters ---------------------\n",
    "topK_display = 12       # Number of before components to display\n",
    "topN_before = 400       # Number of high-energy columns selected from \"before\" for matching\n",
    "topM_after  = 400       # Number of high-energy columns selected from \"after\" for matching\n",
    "chunk_cols = 100        # Chunk size when computing correlation matrix\n",
    "corr_threshold = 0.0    # Matching threshold\n",
    "cmap = 'gray'\n",
    "zoom_padding = 2        # Extra padding around detected patch\n",
    "components_per_image = 6  # Number of components per long image\n",
    "# -----------------------------------------------------\n",
    "\n",
    "def ensure_numpy(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.detach().cpu().numpy()\n",
    "    return np.asarray(x)\n",
    "\n",
    "def l2_norms_of_components(u):\n",
    "    \"\"\"Compute L2 norm for each component\"\"\"\n",
    "    R = u.shape[2]\n",
    "    return np.array([np.linalg.norm(u[:,:,i]) for i in range(R)])\n",
    "\n",
    "def detect_nonzero_bbox(img, threshold_quantile=0.01):\n",
    "    \"\"\"\n",
    "    Detect bounding box of non-zero regions in an image\n",
    "    Returns (row_min, row_max, col_min, col_max)\n",
    "    \"\"\"\n",
    "    img_abs = np.abs(img)\n",
    "    \n",
    "    # Use quantile threshold to determine \"non-zero\" region (noise removal)\n",
    "    threshold = np.quantile(img_abs[img_abs > 0], threshold_quantile) if np.any(img_abs > 0) else 0\n",
    "    mask = img_abs > threshold\n",
    "    \n",
    "    if not np.any(mask):\n",
    "        return 0, img.shape[0], 0, img.shape[1]\n",
    "    \n",
    "    # Find bounding rows and columns\n",
    "    rows = np.any(mask, axis=1)\n",
    "    cols = np.any(mask, axis=0)\n",
    "    \n",
    "    row_indices = np.where(rows)[0]\n",
    "    col_indices = np.where(cols)[0]\n",
    "    \n",
    "    if len(row_indices) == 0 or len(col_indices) == 0:\n",
    "        return 0, img.shape[0], 0, img.shape[1]\n",
    "    \n",
    "    row_min, row_max = row_indices[0], row_indices[-1] + 1\n",
    "    col_min, col_max = col_indices[0], col_indices[-1] + 1\n",
    "    \n",
    "    return row_min, row_max, col_min, col_max\n",
    "\n",
    "def get_union_bbox(img1, img2, padding=2):\n",
    "    \"\"\"\n",
    "    Get union bounding box of two images' non-zero regions\n",
    "    Ensures before and after use the same cropping region\n",
    "    \"\"\"\n",
    "    bbox1 = detect_nonzero_bbox(img1)\n",
    "    bbox2 = detect_nonzero_bbox(img2)\n",
    "    \n",
    "    # Union of boxes\n",
    "    row_min = min(bbox1[0], bbox2[0])\n",
    "    row_max = max(bbox1[1], bbox2[1])\n",
    "    col_min = min(bbox1[2], bbox2[2])\n",
    "    col_max = max(bbox1[3], bbox2[3])\n",
    "    \n",
    "    # Add padding\n",
    "    H, W = img1.shape\n",
    "    row_min = max(0, row_min - padding)\n",
    "    row_max = min(H, row_max + padding)\n",
    "    col_min = max(0, col_min - padding)\n",
    "    col_max = min(W, col_max + padding)\n",
    "    \n",
    "    return row_min, row_max, col_min, col_max\n",
    "\n",
    "def prepare_top_subset(u_before, u_after, topN_before, topM_after):\n",
    "    \"\"\"Select subset from u_before/u_after based on energy for matching\"\"\"\n",
    "    H, W, Rb = u_before.shape\n",
    "    _, _, Ra = u_after.shape\n",
    "    M = H * W\n",
    "\n",
    "    ener_b = l2_norms_of_components(u_before)\n",
    "    ener_a = l2_norms_of_components(u_after)\n",
    "    \n",
    "    nb = min(topN_before, Rb)\n",
    "    na = min(topM_after, Ra)\n",
    "\n",
    "    idx_b = np.argsort(ener_b)[-nb:][::-1]\n",
    "    idx_a = np.argsort(ener_a)[-na:][::-1]\n",
    "\n",
    "    B = u_before.reshape(M, Rb)[:, idx_b]\n",
    "    A = u_after.reshape(M, Ra)[:, idx_a]\n",
    "\n",
    "    return B, A, idx_b, idx_a, ener_b, ener_a\n",
    "\n",
    "def normalize_cols_float32(X):\n",
    "    \"\"\"Normalize column vectors\"\"\"\n",
    "    Xf = X.astype(np.float32, copy=False)\n",
    "    norms = np.linalg.norm(Xf, axis=0).astype(np.float32)\n",
    "    nz = norms > 0\n",
    "    if np.any(nz):\n",
    "        Xf[:, nz] /= norms[nz]\n",
    "    return Xf, norms\n",
    "\n",
    "def compute_corr_blockwise(Bn, An, chunk_cols=100):\n",
    "    \"\"\"Compute correlation in blocks\"\"\"\n",
    "    nb = Bn.shape[1]\n",
    "    na = An.shape[1]\n",
    "    corr = np.empty((nb, na), dtype=np.float32)\n",
    "    for start in range(0, na, chunk_cols):\n",
    "        end = min(start + chunk_cols, na)\n",
    "        corr[:, start:end] = np.dot(Bn.T, An[:, start:end])\n",
    "    return np.abs(corr)\n",
    "\n",
    "def greedy_match_topK_for_befores(corr_abs, top_before_indices_in_subset=None):\n",
    "    \"\"\"Greedy matching\"\"\"\n",
    "    nb, na = corr_abs.shape\n",
    "    if top_before_indices_in_subset is None:\n",
    "        rows_to_process = range(nb)\n",
    "    else:\n",
    "        rows_to_process = list(top_before_indices_in_subset)\n",
    "\n",
    "    matches = []\n",
    "    for r in rows_to_process:\n",
    "        col = int(np.argmax(corr_abs[r, :]))\n",
    "        val = float(corr_abs[r, col])\n",
    "        matches.append((r, col, val))\n",
    "    matches = sorted(matches, key=lambda x: x[2], reverse=True)\n",
    "    return matches\n",
    "\n",
    "def plot_component_grid(matches_data, start_idx, end_idx, cmap='gray', save_path=None):\n",
    "    \"\"\"\n",
    "    Plot a component comparison grid:\n",
    "    Each row: Before | After | Difference\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matches_data: list of tuples (before_cropped, after_cropped, diff, idx_before, idx_after, corr_val, bbox)\n",
    "    start_idx, end_idx: range of matches to display\n",
    "    \"\"\"\n",
    "    num_show = end_idx - start_idx\n",
    "    if num_show <= 0:\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(num_show, 3, figsize=(12, 3*num_show))\n",
    "    \n",
    "    # Handle single row case\n",
    "    if num_show == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, idx in enumerate(range(start_idx, end_idx)):\n",
    "        if idx >= len(matches_data):\n",
    "            break\n",
    "            \n",
    "        b_crop, a_crop, diff, idx_b, idx_a, corr, bbox = matches_data[idx]\n",
    "        row_min, row_max, col_min, col_max = bbox\n",
    "        \n",
    "        # Normalize color range\n",
    "        vmax = max(np.abs(b_crop).max(), np.abs(a_crop).max())\n",
    "        vmin = -vmax\n",
    "        vmax_diff = np.max(np.abs(diff)) if diff.size > 0 else 1\n",
    "        \n",
    "        # Before\n",
    "        im0 = axes[i, 0].imshow(b_crop, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "        axes[i, 0].axis('off')\n",
    "        axes[i, 0].set_title(f'Before #{idx_b}\\nBBox:[{row_min}:{row_max},{col_min}:{col_max}]', \n",
    "                            fontsize=9)\n",
    "        plt.colorbar(im0, ax=axes[i, 0], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # After\n",
    "        im1 = axes[i, 1].imshow(a_crop, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "        axes[i, 1].axis('off')\n",
    "        axes[i, 1].set_title(f'After #{idx_a}\\nCorr={corr:.3f}', fontsize=9)\n",
    "        plt.colorbar(im1, ax=axes[i, 1], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # Difference\n",
    "        im2 = axes[i, 2].imshow(diff, cmap='seismic', vmin=-vmax_diff, vmax=vmax_diff)\n",
    "        axes[i, 2].axis('off')\n",
    "        mse = np.mean(diff**2)\n",
    "        axes[i, 2].set_title(f'Diff (After-Before)\\nMSE={mse:.2e}', fontsize=9)\n",
    "        plt.colorbar(im2, ax=axes[i, 2], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=200, bbox_inches='tight')\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# ---------------------------- Main Pipeline ----------------------------\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 1: PREPARING DATA AND MATCHING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "u_before = ensure_numpy(u_before)\n",
    "u_after  = ensure_numpy(u_after)\n",
    "\n",
    "if u_before.ndim != 3 or u_after.ndim != 3:\n",
    "    raise ValueError(\"u_before/u_after must have shape (H, W, R)\")\n",
    "\n",
    "H, W, Rb = u_before.shape\n",
    "H2, W2, Ra = u_after.shape\n",
    "assert (H, W) == (H2, W2), \"Spatial sizes must match\"\n",
    "\n",
    "print(f\"Original ranks: before Rb={Rb}, after Ra={Ra}\")\n",
    "\n",
    "# Prepare matching subset\n",
    "B, A, idx_b, idx_a, ener_b, ener_a = prepare_top_subset(\n",
    "    u_before, u_after, topN_before, topM_after\n",
    ")\n",
    "M = H * W\n",
    "print(f\"Selected subsets for matching: before nb={B.shape[1]}, after na={A.shape[1]} (M={M})\")\n",
    "\n",
    "# Normalize\n",
    "Bn, norms_b = normalize_cols_float32(B)\n",
    "An, norms_a = normalize_cols_float32(A)\n",
    "\n",
    "# Compute correlation\n",
    "est_bytes = Bn.shape[1] * An.shape[1] * 4\n",
    "print(f\"Estimated correlation matrix size: {est_bytes/1e6:.1f} MB. chunk_cols={chunk_cols}\")\n",
    "\n",
    "corr_abs = compute_corr_blockwise(Bn, An, chunk_cols=chunk_cols)\n",
    "print(\"Correlation computed.\")\n",
    "\n",
    "# Matching\n",
    "topK_display_actual = min(topK_display, B.shape[1])\n",
    "rows_to_match = list(range(topK_display_actual))\n",
    "matches = greedy_match_topK_for_befores(corr_abs, top_before_indices_in_subset=rows_to_match)\n",
    "\n",
    "# Map back to original indices\n",
    "matches_mapped = []\n",
    "for r_sub, c_sub, val in matches:\n",
    "    orig_b = int(idx_b[r_sub])\n",
    "    orig_a = int(idx_a[c_sub])\n",
    "    matches_mapped.append((orig_b, orig_a, val))\n",
    "\n",
    "print(\"\\nMatching results:\")\n",
    "for b, a, v in matches_mapped[:topK_display_actual]:\n",
    "    print(f\"  Before #{b:4d} -> After #{a:4d}   Corr={v:.4f}\")\n",
    "\n",
    "# ---------------------------- Prepare Cropped Data ----------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: CROPPING COMPONENTS TO PATCHES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "matches_data = []\n",
    "for orig_b, orig_a, val in matches_mapped[:topK_display_actual]:\n",
    "    if val < corr_threshold:\n",
    "        continue\n",
    "    \n",
    "    before_img = u_before[:, :, orig_b]\n",
    "    after_img = u_after[:, :, orig_a]\n",
    "    \n",
    "    # Align sign\n",
    "    if np.sum(before_img.ravel() * after_img.ravel()) < 0:\n",
    "        after_img = -after_img\n",
    "    \n",
    "    # Get union bounding box\n",
    "    bbox = get_union_bbox(before_img, after_img, padding=zoom_padding)\n",
    "    row_min, row_max, col_min, col_max = bbox\n",
    "    \n",
    "    # Crop\n",
    "    b_crop = before_img[row_min:row_max, col_min:col_max]\n",
    "    a_crop = after_img[row_min:row_max, col_min:col_max]\n",
    "    diff = a_crop - b_crop\n",
    "    \n",
    "    matches_data.append((b_crop, a_crop, diff, orig_b, orig_a, val, bbox))\n",
    "    print(f\"  Component {len(matches_data)}: Before #{orig_b} -> After #{orig_a}, \"\n",
    "          f\"Cropped size: {b_crop.shape}, BBox: [{row_min}:{row_max}, {col_min}:{col_max}]\")\n",
    "\n",
    "# ---------------------------- Create Long Images ----------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: CREATING COMPONENT COMPARISON GRID IMAGES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "num_images = int(np.ceil(len(matches_data) / components_per_image))\n",
    "\n",
    "for img_idx in range(num_images):\n",
    "    start_idx = img_idx * components_per_image\n",
    "    end_idx = min((img_idx + 1) * components_per_image, len(matches_data))\n",
    "    \n",
    "    print(f\"\\nCreating image {img_idx+1}/{num_images} (components {start_idx+1}-{end_idx})...\")\n",
    "    \n",
    "    save_path = f'component_grid_{img_idx+1}_of_{num_images}.png'\n",
    "    plot_component_grid(matches_data, start_idx, end_idx, cmap=cmap, save_path=save_path)\n",
    "\n",
    "\n",
    "print(\"\\nAll visualizations complete!\")\n",
    "print(f\"\\nGenerated {num_images} component grid image(s)\")\n",
    "print(\"Files:\")\n",
    "for i in range(num_images):\n",
    "    print(f\"  - component_grid_{i+1}_of_{num_images}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e93e89-86b4-432b-b3c8-dc7c54881f33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311v2",
   "language": "python",
   "name": "py311v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
